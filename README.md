# üö¶ Detecci√≥n de Se√±ales de Tr√°nsito con YOLOv8

Proyecto final de la asignatura **Procesamiento Digital de Im√°genes (2025-2)**.  
El objetivo es entrenar y desplegar un modelo de **detecci√≥n de objetos** capaz de localizar y clasificar diferentes **se√±ales de tr√°nsito** en im√°genes.

> **Autores:** Juan Camilo Mi√±o Castillo, Luis Eduardo Mi√±o Castillo
> **Profesor:** Lucas Miguel Iturriago Salas 
> **Curso:** PDI ‚Äì 2025-2

---

## 1. Descripci√≥n del proyecto

En este proyecto se entrena un modelo **YOLOv8** para detectar distintas se√±ales de tr√°nsito (por ejemplo l√≠mites de velocidad, se√±ales de prohibici√≥n, etc.) en im√°genes RGB.

El pipeline completo incluye:

1. **Preparaci√≥n del dataset** desde Roboflow (formato YOLOv8).
2. **Entrenamiento** del modelo en Google Colab.
3. **Evaluaci√≥n** del desempe√±o (mAP, matriz de confusi√≥n, curva F1‚Äìconfianza).
4. **Exportaci√≥n del modelo** a formato **TorchScript**.
5. **Despliegue** del modelo en un **HuggingFace Space** mediante FastAPI + Docker.
6. **Inferencia local** en Python (carga directa del modelo).
7. **Inferencia remota v√≠a API** consumiendo el Space de HuggingFace.

---

## 2. Dataset

- **Origen:** Roboflow  
- **Tarea:** Detecci√≥n de objetos (YOLOv8)  
- **N√∫mero de im√°genes:** = 4.720  
- **Split:** train / valid / test  
- **Preprocesamiento principal:**
  - Resize a **512√ó512**
  - Auto-orientaci√≥n de im√°genes

- **Clases (ejemplos):**
  - L√≠mites de velocidad (10, 20, 30, 40, 60, ‚Ä¶)
  - Se√±al de prohibici√≥n (stop)
  - Color del sem√°foro (verde, rojo)

üëâ **Enlace al dataset (Roboflow):**  
`https://app.roboflow.com/universidad-nacional-o6onq/detect-project-lqv4y/models` 

---

## 3. Estructura del repositorio

```text
.
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ modelo  # Notebook principal: entrenamiento + m√©tricas + TorchScript + ejemplos  Modelo exportado a TorchScript + HuggingFace
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ inferencia.ipynb                    # Inferencia consumiendo la API en HuggingFace
‚îú‚îÄ‚îÄ recursos/ # Videos e imagenes utilizadas para probar el modelo
‚îÇ
‚îú‚îÄ‚îÄ PDI_PRESENTACION_FINAL.pdf
‚îú‚îÄ‚îÄ requirements.txt                    # Dependencias de Python
‚îî‚îÄ‚îÄ README.md
````

---

## 4. Requisitos

* Python 3.10+
* Paquetes principales:

  * `ultralytics`
  * `torch`, `torchvision`
  * `opencv-python`
  * `matplotlib`
  * `fastapi`, `uvicorn` (para la API)
  * `requests`

Instalaci√≥n r√°pida:

```bash
pip install -r requirements.txt
```

Si solo quieres probar **inferencia local**:

```bash
pip install ultralytics opencv-python matplotlib
```

---

## 5. Entrenamiento del modelo (YOLOv8)

El entrenamiento se realiza en el notebook:

* `notebooks/modelo.ipynb` 
Pasos principales en el notebook:

1. Descargar el dataset desde Roboflow (formato YOLOv8).
2. Definir hiperpar√°metros:

   * Modelo base (por ejemplo `yolov8n.yaml`)
   * `imgsz`, `epochs`, `batch`, etc.
3. Entrenar:

   * `model.train(data=data_yaml, ...)`
4. Visualizar resultados:

   * P√©rdidas de entrenamiento.
   * mAP50 y mAP50-95.
   * Ejemplos de detecci√≥n sobre im√°genes de validaci√≥n.

El notebook genera autom√°ticamente:

* `results.csv` con la evoluci√≥n de m√©tricas.
* `confusion_matrix.png`
* `F1_curve.png`

---

## 6. Exportaci√≥n a TorchScript

En el mismo notebook se exporta el modelo entrenado a **TorchScript** usando Ultralytics:

```python
from ultralytics import YOLO

model = YOLO("ruta/al/best.pt")
exported_file = model.export(
    format="torchscript",
    imgsz=640,
    optimize=False   # para evitar problemas con xnnpack
)
```

Esto genera un archivo tipo:

```text
signs_detection/yolov8n_signsX/weights/best.torchscript
```

Ese archivo se copia como:

```text
models/traffic_signs_yolo.torchscript
```

Adem√°s, en el notebook se comparan:

* Tiempos de inferencia del modelo `.pt` vs `.torchscript`.
* Resultados visuales sobre la misma imagen.

---

## 7. Inferencia local (script `infer_local.py`)

Este script carga el modelo YOLO (`.pt` o `.torchscript`) desde disco, ejecuta la detecci√≥n sobre una imagen y guarda el resultado con las cajas dibujadas.

Uso:

```bash
python scripts/infer_local.py \
    --model models/traffic_signs_yolo.torchscript \
    --image data/ejemplo.jpg \
    --conf 0.4 \
    --imgsz 640 \
    --output outputs/local_prediction.jpg
```

Par√°metros:

* `--model`: ruta al modelo entrenado (`.pt` o `.torchscript`).
* `--image`: ruta a la imagen de entrada.
* `--conf`: umbral de confianza.
* `--imgsz`: tama√±o de la imagen de entrada.
* `--output`: ruta donde se guardar√° la imagen con las cajas.

---

## 8. Despliegue en HuggingFace Space

El modelo se despliega en un **Space** de tipo **Docker** usando FastAPI.

* **Space:** `Camilosss/TrafficSignDetectionYOLO`
* **URL p√∫blica:**
  `https://camilosss-trafficsigndetectionyolo.hf.space`

Dentro del Space se incluyen los archivos:

* `app.py` ‚Äì API REST (FastAPI) con endpoints:

  * `GET /` ‚Äì mensaje de bienvenida.
  * `GET /health` ‚Äì estado del modelo.
  * `POST /predict` ‚Äì recibe una imagen en base64 y devuelve detecciones en JSON.
* `traffic_signs_yolo.torchscript` ‚Äì modelo exportado.
* `requirements.txt`, `runtime.txt`, `Dockerfile`, `README.md`.

Ejemplo de respuesta de `/predict`:

```json
{
  "num_detections": 1,
  "detections": [
    {
      "class_id": 5,
      "class_name": "30",
      "confidence": 0.94,
      "x1": 177.2,
      "y1": 16.3,
      "x2": 406.2,
      "y2": 243.4
    }
  ],
  "image_size": [315, 474]
}
```

---

## 9. Inferencia v√≠a API (script `infer_api.py`)

Este script:

1. Lee una imagen local.
2. La codifica en **base64**.
3. Env√≠a un `POST` al endpoint `/predict` del Space.
4. Imprime el JSON de salida.
5. Dibuja las cajas en la imagen y la guarda en disco.

Uso:

```bash
python scripts/infer_api.py \
    --image data/ejemplo.jpg \
    --url https://camilosss-trafficsigndetectionyolo.hf.space/predict \
    --conf 0.4 \
    --output outputs/api_prediction.jpg
```

Par√°metros:

* `--image`: ruta a la imagen local.
* `--url`: URL del endpoint `/predict` del Space.
* `--conf`: umbral de confianza.
* `--output`: ruta de la imagen con cajas dibujadas.

---

## 10. Resultados y m√©tricas

En el notebook se reportan:

* **mAP50** y **mAP50-95** sobre el conjunto de validaci√≥n.
* **Matriz de confusi√≥n** normalizada por clase.
* **F1‚ÄìConfidence curve** indicando el umbral √≥ptimo de confianza.

Adem√°s, se muestran:

* Ejemplos de detecci√≥n sobre im√°genes de validaci√≥n.
* Ejemplo de detecci√≥n local (modelo TorchScript).
* Ejemplo de detecci√≥n v√≠a API (Space de HuggingFace).

---

## 11. Limitaciones y trabajo futuro

* El modelo se entren√≥ con im√°genes principalmente bien iluminadas; podr√≠a fallar con:

  * Escenas nocturnas.
  * Se√±ales parcialmente tapadas.
  * Se√±ales muy lejanas o desenfocadas.
* No se ha optimizado todav√≠a para correr en tiempo real en un sistema embebido (Raspberry Pi, etc.).
* Podr√≠a ampliarse:

  * A√±adiendo nuevas clases de se√±ales.
  * Probando modelos YOLO m√°s grandes (`yolov8s`, `yolov8m`) para mayor precisi√≥n.
  * Integrando seguimiento de objetos (tracking) en video.

---

## 12. C√≥mo reproducir (resumen r√°pido)

1. **Clonar el repo** y crear entorno:

```bash
git clone https://github.com/<usuario>/<repo>.git
cd <repo>
pip install -r requirements.txt
```

2. **Entrenar / revisar entrenamiento**
   Abrir `notebooks/01_yolov8_traffic_signs.ipynb` en Colab o Jupyter y ejecutar.

3. **Inferencia local con imagen:**

```bash
python scripts/infer_local.py --model models/traffic_signs_yolo.torchscript --image data/ejemplo.jpg
```

4. **Inferencia v√≠a API (Space HF):**

```bash
python scripts/infer_api.py --image data/ejemplo.jpg
```

---

Cualquier duda o mejora futura (nuevas clases, otros modelos, despliegue en dispositivos embebidos) se puede documentar en issues o forks del repositorio.

